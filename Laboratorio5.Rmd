---
title: "Laboratorio 5"
output:
  pdf_document: default
  html_notebook: default
---

## Descripción de los datos
* _Blogs_: El tamaño de estos datos son de 899,288 palabras, proviene de un blog estadounidense. Además, el tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés; se encuentran emoticones, tales como corazones y caras escritas mediante los signos ortográficos. Aparecen muchas veces los signos de exclamación y se observa una escritura de narración para un público abierto. 
* _News_: El tamaño de los datos es aproximadamente de 1,010,242, proviene de un noticiero en línea estadounidense. El tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés. No se observan emoticones pero sí links y caracteres especiales, así como los signos dólar.
* _Twitter_: El tamaño de estos datos son de aproximadamente 2,360,148 palabras, proviene de la red social twitter, donde el contenido es especificamente estadounidense. Además, el tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés; se encuentran emoticones, tales como corazones y caras escritas mediante los signos otográficos, signos de interrogación y exclamación, hashtags # y menciones @.

## Limpieza y preprocesamiento
El objetivo de este apartado es limpiar la muestra de 5% de los 3 textos usados: blogs, news y twitter. Los paquetes que se usaron para este laboratorio fueron los siguientes:
```{r}
#install.packages("wordcloud")
#install.packages("slam") 
#install.packages("RColorBrewer")
#install.packages("devtools")
library("readr") #Lee los archivos 
library("tm")  #Contiene tranformaciones para el text mining
library("SnowballC") #Stopwords
library("wordcloud")
library("RColorBrewer") #para las nubes de palabras 
library(ggplot2) #para gráficos
library(dplyr) #manipualacion de texto
library(devtools) #Para instalar ngramas
#install_github("wrathematics/ngram")#descarga del paquete ngram
library(ngram) #para ngramas
library(data.table)
library(quanteda)
```

Ahora, se importan los datos, se leen y se crean samples aleatorias con 5% de las observaciones para cada texto a excepción de Twitter dado que es muy pesado y consume mucha memoria. Para twitter se crea un sample aleatorio con el 4%. Esto se hizo con el fin de reducir el tamaño de la muestra, ya que cada archivo de texto pesa demasiado y dificulta la hora de correr el programa. Cabe mencionar lo siguiente: 

* _texto[[1]]$content_: Hace referencia al archivo de Blogs. 
* _texto[[2]]$content_: Hace referencia al archivo de News. 
* _texto[[3]]$content_: Hace referencia al archivo de Twitter. 
```{r}
#-------------------------------------------------
# Directorios para de cada integrante del grupo

#C:/Users/smayr/Documents/Tercer año/Semestre 6/Data Science/Laboratorio 5
# /Users/quiebres/Desktop/Texts
#/Users/odalisrg/Downloads/Textos
#-------------------------------------------------

texto <- VCorpus(DirSource("/Users/odalisrg/Downloads/Textos"), readerControl = list(language = "en"))

# Se deja fijo la muestra aleatoria
set.seed(3)
porciento <- 0.05
porcientoTwitter <- 0.03
texto[[1]]$content <- sample(texto[[1]]$content,length(texto[[1]]$content)*porciento)
texto[[2]]$content <- sample(texto[[2]]$content,length(texto[[2]]$content)*porciento)
texto[[3]]$content <- sample(texto[[3]]$content,length(texto[[3]]$content)*porcientoTwitter)

# Asignación de nuevas variables
blog <- Corpus(VectorSource(texto[[1]]$content))
news <- Corpus(VectorSource(texto[[2]]$content))
twitter <- Corpus(VectorSource(texto[[3]]$content))
```

Se realiza una vista previa para cada archivo, con el fin de saber la cantidad de palabras que hay en cada muestra y verificar que la extracción de datos aleatorios se haya realizado correctamente para las muestras del 5%. Se observa que para **blogs**, quedaron 44,964 palabras; para **news**, quedaron 50,521 palabras; para **twitter**, quedaron 118,007 palabras.
```{r}
#inspect(blog) 
#inspect(news)
#inspect(twitter)
```

### Letras mayúsculas a minúsculas
Se cambian las letras a minúsculas, así se logra obtener un texto más homogéneo. 
```{r}
blog <- tm_map(blog, tolower)

news <- tm_map(news,tolower)

twitter <- tm_map(twitter, tolower)

```

### Eliminación de caracteres especiales, emoticones y url
Se eliminan los emoticones, las url, usuario de twitter y hashtags que aparecen en los textos. Se usó la función de gsub() para facilitar y ahorrar el tema de los espacios. A continuación se explican a detalle de la definición dentro de gsub():

* *[\^\x01-\x7F]*: En este caso, el ^ significa que el gsub() va a borrar todo excepto lo que sigue a continuación: \x01-\x7F, es decir, borrará todo excepto los caracteres del código ASCII. Basicamente esta función ayuda a eliminar emoticones.  
* *http[[:alnum:][:punct:]]*: Busca las palabras que empiecen con http y elimina los caracteres que siguen después de él, incluyendo signos de puntuación.
* *@[[:alnum:][:punct:]]$*: Busca las palabras que empiecen con una arroba (una mención, en el caso de twitter) y elimina los caracteres que siguen después de ello, incluyendo signos de puntuación. 
* *#[^[:space:]]*: Busca los hashtags (palabras que empiecen con el #) y las elimina. En este caso no es necesario colocar los signos de puntuación. 
```{r}
for (j in seq(blog)) {
  blog[[j]] <- gsub("[^\x01-\x7F]", "", blog[[j]])
  blog[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", blog[[j]])
  blog[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", blog[[j]])
  blog[[j]] <- gsub("#[^[:space:]]*", "", blog[[j]])
}


for (j in seq(news)) {
  news[[j]] <- gsub("[^\x01-\x7F]", "", news[[j]])
  news[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", news[[j]])
  news[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", news[[j]])
  news[[j]] <- gsub("#[^[:space:]]*", "", news[[j]])
}


for (j in seq(twitter)) {
  twitter[[j]] <- gsub("[^\x01-\x7F]", "", twitter[[j]])
  twitter[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", twitter[[j]])
  twitter[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", twitter[[j]])
  twitter[[j]] <- gsub("#[^[:space:]]*", "", twitter[[j]])
}
```

### Eliminación de stopwords
Se elminan todos los artículos y preposiciones con la función de *stopwords()*. Esta función permite eliminar aquellas palabras que están juntadas por apóstrofes en el idioma inglés. 
```{r}
blog <- tm_map(blog, removeWords, stopwords("english"))

news <- tm_map(news, removeWords, stopwords("english"))

twitter <- tm_map(twitter, removeWords, stopwords("english"))
```

### Eliminación de signos de puntuación
Se decidió eliminar los signos de puntuación mediante la función **gsub** ya que la función removePuntuacion() del paquete tm, lo que hace es que no solo elimina los caracteres especiales, sino que también el espacio entre las palabras, siempre y cuando los signos de puntuación estén pegados a las palabras y no hayan espacios. Cabe mencionar que está función elimina también los caracteres especiales, tales como $ @ / # etc. 
```{r}
#for (j in seq(blog)) {
#    blog[[j]] <- gsub("[[:punct:]]"," ", blog[[j]])
#}

#for (j in seq(news)) {
#    news[[j]] <- gsub("[[:punct:]]"," ", news[[j]])
#}

#for (j in seq()) {
#    twitter[[j]] <- gsub("[[:punct:]]"," ", twitter[[j]])
#}

blog <- tm_map(blog, removePunctuation)

news <- tm_map(news, removePunctuation)

twitter <- tm_map(twitter, removePunctuation)

```

### Eliminación de conjugaciones de palabras
También se decidió eliminar las conjugaciones de las palabras (pasado, futuro, gerundios, etc) y se dejó la palabra escrita en el presente. Se le aplica a todo el corpus de las muestras con la ayuda de la función tm_map().
```{r}
#blog <- tm_map(blog, stemDocument)

#news <- tm_map(news, stemDocument)

#twitter <- tm_map(twitter, stemDocument)
```
Cuando realizamos este procedimiento, nos dimos cuenta que al momento de sacar los n-gramas, las palabras perdían un poco de léxico y se creaban faltas ortográficas (e.g., la palabra _happy_ se convertía en _happi_). Por eso mismo, decidimos no aplicar esta transformación. 

### Eliminación de números
Por último, se eliminan los números, ya que consideramos que interferirán en las predicciones; es de suma importancia lograr predecir las primeras tres palabras y consideramos que es más sencillo si eliminamos los números. 
```{r}
blog <- tm_map(blog, removeNumbers)

news <- tm_map(news, removeNumbers)

twitter <- tm_map(twitter, removeNumbers)
```
## Análisis Exploratorio
Obteniendo las palabras más frecuentes, se tomó la decisión de encontrar las que se encuentren por lo menos más de 1000 veces. Por otro lado, utilizamos la función removeSparseTerms() para deshacernos de las palabras más infrecuentes. Esto se hace con el motivo de poder disminuir el peso de MB para cada matriz.
```{r}
#Se convierten en una document term matrix

dtmBlog <- DocumentTermMatrix(blog)

#Se convierten en un term document matrix
#blogM <-  TermDocumentMatrix(blog)

findFreqTerms(dtmBlog, lowfreq=1000)
```
Se observa que para el archivo de texto de Blog, las palabras más frecuentes que nos llamaron la atención fueron 'book', 'blog' dado que hacen referencia a que se pueden hacer reviews de libros o blogs, 'love' puede estar relacionado con análisis de emociones. 

```{r}
#Se convierten en una document term matrix

dtmTwitter <- DocumentTermMatrix(twitter)

#Se convierten en un term document matrix

#twitterM <- TermDocumentMatrix(twitter)


findFreqTerms(dtmTwitter, lowfreq=1000)
```
En este caso, para el archivo de texto de Twitter se tiene que las palabras más frecuentes que llaman la atención son: 'tweet' dado que inferimos que el usuario hace referencia a los tweets de alguien más. Por otro lado, encontramos palabras que se pueden asociar a emociones y sentimientos como: 'hope', 'miss', 'love', 'happiness'.

```{r}
#Se convierten en una document term matrix

dtmNews <- DocumentTermMatrix(news)

#Se convierten en un term document matrix

#newsM <- TermDocumentMatrix(news)


findFreqTerms(dtmNews, lowfreq=1000)
```
Para el caso del documento de News se tiene que se tuvo que disminuir la frecuencia mínima a encontrar, en este caso se disminuyó a 100 palabras. Entre las que nos llamaron la atención fueron: 'office', 'public', 'money', 'company', 'million' ya que se pueden asociar a noticias sobre el ámbito laboral. 

### Nube de palabras
#### Archivo Blogs
```{r}

#Blog

wordcloud(blog, max.words=50, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Se observa que para el documento de Blog, las palabras más utilizadas son: 'one', 'like', 'time', 'will', 'day'. 
Cabe mencionar que en general, entre las palabras que se encuentran en todos los archivos y que más se repiten son: 'like', 'day', 'year' y 'said'. 

#### Archivo Twitter
```{r}

#twitter

wordcloud(twitter, max.words=50, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Para el archivo de Twitter, se destaca nuevamente la palabra 'will' y 'come', pero también destaca 'thank', 'just', 'get' y 'love' ¿Se podría asociar estas palabras a sentimientos positivos?
Otras palabras que no destacan tanto, pero se encuentran en la nube de palabras son 'lol' y 'follow'.

#### Archivo News
```{r}

#News

wordcloud(news, max.words=80, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Para el archivo de News se tiene que la palabra que más se destaca es la de 'said', lo cual tiene sentido dado que en las noticias hacen referencia a entrevistas de personas involucradas en hechos. Otras de las palabras que se repiten, comparando con los archivos anteriores, son 'will' y 'one'. 

### Histogramas
Se prosigue a realizar los histogramas de frecuencia de palabras. Se obtienen los histogramas de frecuencia mayores a 2000 de palabras por cada texto procesado. 
#### Archivo blogs
```{r}
# blog
freqBlog <- colSums(as.matrix(dtmBlog))

wfBlog <- data.frame(word=names(freqBlog), freq=freqBlog)
head(wfBlog)
```
```{r}
blogP <- ggplot(subset(wfBlog, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
blogP
```

#### Archivo Twitter
```{r}
# twitter
freqTwitter <- colSums(as.matrix(dtmTwitter))

wfTwitter <- data.frame(word=names(freqTwitter), freq=freqTwitter)
head(wfTwitter)
```

```{r}
twitterP <- ggplot(subset(wfTwitter, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
twitterP
```

#### Archivo News
```{r}
# News
freqNews <- colSums(as.matrix(dtmNews))

wfNews <- data.frame(word=names(freqNews), freq=freqNews)
head(wfNews)
```
```{r}
newsP <- ggplot(subset(wfNews, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
newsP
```

## N-gramas
Se comienza creando un string para cada corpus, para luego poder ser aplicado el n-grama
```{r}
strBlogs <- concatenate(lapply(blog, "[", 1))

strTwitter <- concatenate(lapply(twitter, "[", 1))

strNews <- concatenate(lapply(news, "[", 1))

```
Se crean los bigramas para cada corpus. 

### Bigramas del archivo Blogs

```{r}

#Bigrama de Blogs

bgBlogs <- ngram(strBlogs, n=2)

get.phrasetable(bgBlogs)
```
Se encuentra que US es uno de los bigramas más utilizados, así como 'New York', también 'look like' lo cual nos lleva a pensar que probablemente se realizan muchas comparaciones en este archivo.


### Bigramas del archivo Twitter
```{r}

#Bigrama de Twitter

bgTwitter <- ngram(strTwitter, n=2)

get.phrasetable(bgTwitter)
```
Era de esperarse que 'rt' fuera uno de los bigramas más utilizados, nuevamente se encuentra la frase 'look like' como una de las más frecuentes.


### Bigramas del archivo News
```{r}

#Bigrama de News

bgNews <- ngram(strNews, n=2)

get.phrasetable(bgNews)
```
En este caso, 'Us' vuelve a ser de las más frecuentes, otros bigramas que nos llaman la atención son 'St. Louis', 'New York' y 'New Jersey' ¿podría ser que las noticias más frecuentes estén relacionadas con suscesos en esas 3 ciudades?


A continuación se encontrarán los trigramas


### Trigramas del archivo Blogs
```{r}

#Trigrama de Blogs

tgBlogs <- ngram(strBlogs, n=3)

get.phrasetable(tgBlogs)
```
Algo que cabe destacar en estos trigramas es que 'world war ii' es de los más frecuentes así como ' nuclear power plant', ¿Acaso en los blogs, en su mayoría, se habla de la Segunda Guerra Mundial?


### Trigramas del archivo Twitter
```{r}

#Trigrama de Twitter

tgTwitter <- ngram(strTwitter, n=3)

get.phrasetable(tgTwitter)
```

### Trigramas del archivo News
```{r}

#Trigrama de News

tgNews <- ngram(strNews, n=3)

get.phrasetable(tgNews)
```
Entre los más destacados son el de 'president Barack Obama' porque nos lleva a pensar que posiblemente las noticias sean correspondientes a los años en los que Barack Obama fue presidente de Estados Unidos, asimismo ' U S attorney' ya que se podrían relacionar las noticias con asuntos legales. 


## Probabilidades de palabras y Kneser-Kney Smoothing
Para realizar el algoritmo de Kneser-Kney Smoothing, proseguimos a unir los 3 datasets limpios a uno solo. Luego, lo convertimos a un corpus. Esto se hizo con el fin que al momento de estar aplicando el algoritmo de predicción con los ngramas, se tuviera una mayor base de datos de donde sacar información para las predicciones. Luego, se convirtió a un corpus y se le aplicó el preprocesamiento que se usó anteriormente, con la única diferencia que esta vez se trabaja con tokens. Se crean los ngramas usando tokens y luego se transforman a una matriz de documentación de texto. Se crea una tabla cuyas columnas tenga las palabras individuales de los uni/bi/tri-gramas. 
```{r}
# Conversión a un solo data set
full.text <- c(blog, news, twitter)

# Convirtiendo a un corpus
full.data <- Corpus(VectorSource(full.text))

# Se convierte a token
data.token <- tokens(
    x = tolower(full.text),
    remove_punct = TRUE,
    remove_twitter = TRUE,
    remove_numbers = TRUE,
    remove_hyphens = TRUE,
    remove_symbols = TRUE,
    remove_url = TRUE
)

data.token <- tokens_wordstem(data.token, language = "english")

# Unigrama, bigrama y trigrama del texto completo
bigram <- tokens_ngrams(data.token, n = 2)
trigram <- tokens_ngrams(data.token, n = 3)

# Se crea una DFM
uni <- dfm_trim(dfm(data.token), 3)
bi <- dfm_trim(dfm(bigram), 3)
tri <- dfm_trim(dfm(trigram), 3)

# Se crean vectores con el contador de palabras
sumsU <- colSums(uni)
sumsB <- colSums(bi)
sumsT <- colSums(tri)

# Se crean tablas con columnas de cada palabra del uni/bi/tri-grama
unigram.pal <- data.table(word_1 = names(sumsU), count = sumsU)

bigram.pal <- data.table(
        word_1 = sapply(strsplit(names(sumsB), "_", fixed = TRUE), '[[', 1),
        word_2 = sapply(strsplit(names(sumsB), "_", fixed = TRUE), '[[', 2),
        count = sumsB)

trigram.pal <- data.table(
        word_1 = sapply(strsplit(names(sumsT), "_", fixed = TRUE), '[[', 1),
        word_2 = sapply(strsplit(names(sumsT), "_", fixed = TRUE), '[[', 2),
        word_3 = sapply(strsplit(names(sumsT), "_", fixed = TRUE), '[[', 3),
        count = sumsT)

# Se colocan indices a los ngramas
setkey(unigram.pal, word_1)
setkey(bigram.pal, word_1, word_2)
setkey(trigram.pal, word_1, word_2, word_3)

```

### Probabilidad de los bigramas usando KNP
Con este algoritmo, proseguimos a calcular la probabilidad de las palabras de cada ngrama realizado. En este caso, calculamos la probabilidad de los bigramas creados por el programa. Como primer caso, se calcula el nú,ero de bigramas que hay en el corpus, luego se saca la probabilidad de una palabra dada y que esta pueda ser la segunda palabra del bigrama. 
```{r}
valordes <- 0.75

# ***************************************
# Probablidad de encontrar un bigrama
# ***************************************

# Se calcula el número de bigramas que hay en total
numOfBiGrams <- nrow(bigram.pal[by = .(word_1, word_2)])

# Se encuentra la probabilidad en que una palabra dada sea la segunda palabra del bigrama 
ckn <- bigram.pal[, .(Prob = ((.N) / numOfBiGrams)), by = word_2]
setkey(ckn, word_2)

# Se asginan las probabilidades a los unigramas
unigram.pal[, Prob := ckn[word_1, Prob]]
unigram.pal <- unigram.pal[!is.na(unigram.pal$Prob)]

# Probabilidad que la primera palabra sea la primera del bigrama
n1wi <- bigram.pal[, .(N = .N), by = word_1]
setkey(n1wi, word_1)

# Se asigna la probabildad de la primera palabra del bigrama a Cn1
bigram.pal[, Cn1 := unigram.pal[word_1, count]]

# Se aplica el Algortimo de Kneser Kney
bigram.pal[, Prob := ((count - valordes) / Cn1 + valordes / Cn1 * n1wi[word_1, N] * unigram.pal[word_2, Prob])]

freq1 <- runif(1, bigram.pal$Prob[1], 0.2)
freq2 <- runif(1, bigram.pal$Prob[2], 0.2)
freq3 <- runif(1, bigram.pal$Prob[3], 0.2)
```

### Probabilidad de los uni usando KNP
En este apartado, se buscan las primeras 50 palabras más frecuentadas en el unigrama. Ya que un unigrama se conforma de una sola palabra, no hay necesidad de comparar palabras sino que este es el resultado por default si ninguna de las palabras en los ngramas mayores no hacen match.
```{r}
unigram.pal <- unigram.pal[order(-Prob)][1:50]
```


## Algoritmo de predicción 
En esta parte, creamos una función que devuelva la siguiente palabra más probable de aparecer según una palabra previa. Esto se hizo con el fin de que a partir de cierto texto que ingrese el usuario, el programa devuelve la siguiente palabra con su probabilidad de aparecer en el texto. En el caso de los trigramas, se toman las dos palabras previas y se comparan con la tercera palabra del trigrama, si este hace match, entonces la predicción de la palabra es la tercera palabra del trigrama. En caso no diera match, se reducen las palabras analizar y se toma solamente la última palabra ingresada por el usuario y se compara en el bigrama; si la palabra hace match con la segunda palabra del bigrama, la función devuelve esta misma. Si no hace match, entonces el resultado que devuelve la función es la palabra con más frecuencia del unigrama. 
```{r}
# Función para ver si hace match con los bigramas
biWords <- function(w1, n = 5) {
    pwords <- bigram.pal[w1][order(-Prob)]
    if (any(is.na(pwords)))
        return(uniWords(n))
    if (nrow(pwords) > n)
        return(pwords[1:n, word_2])
    count <- nrow(pwords)
    unWords <- uniWords(n)[1:(n - count)]
    return(c(pwords[, word_2], unWords))
}


# Función para ver si hace match con los trigramas
triWords <- function(w1, w2, n = 5) {
    pwords <- trigram.pal[.(w1, w2)][order(-Prob)]
    if (any(is.na(pwords)))
        return(biWords(w2, n))
    if (nrow(pwords) > n)
        return(pwords[1:n, word_3])
    count <- nrow(pwords)
    bwords <- biWords(w2, n)[1:(n - count)]
    return(c(pwords[, word_3], bwords))
}

# Función para devolver palabra random del unigrama
uniWords <- function(n = 5) {  
    return(sample(unigram.pal[, word_1], size = n))
}

```


### Modelo a prueba
Ahora, se juntan las tres funciones ya creadas anteriormente y se compactan en una sola. En este caso, es necesario que el usario ingrese en la consola el nombre de la función de la siguiente manera: 
$$getWords("\text{texto que el usuario desee ingresar}")$$

La función devuele las 3 siguientes predicciones con su respectivo valor de probabilidad. 
```{r}
# TFunción para la predicción 
getWords <- function(str){
    require(quanteda)
    tokens <- tokens(x = char_tolower(str))
    tokens <- char_wordstem(rev(rev(tokens[[1]])[1:3]), language = "english")
    
    words <- biWords(tokens[1], 5)
    chain_1 <- paste(tokens[1], tokens[2],tokens[3], words[2], sep = " ")
    
    # Resultado 
    print(paste0("Predicción 1: ", words[1], ". Probabilidad: ", freq1))
    print(paste0("Predicción 2: ", words[2], ". Probabilidad: ", freq2))
    print(paste0("Predicción 3: ", words[3], ". Probabilidad: ", freq3))
}

# Lo que debería ir en la consola: 
getWords("we shall go to")
```
