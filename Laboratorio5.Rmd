---
title: "Laboratorio 5"
output:
  html_notebook: default
  pdf_document: default
---

## Descripción de los datos
* _Blogs_: El tamaño de estos datos son de 899,288 palabras, proviene de un blog estadounidense. Además, el tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés; se encuentran emoticones, tales como corazones y caras escritas mediante los signos ortográficos. Aparecen muchas veces los signos de exclamación y se observa una escritura de narración para un público abierto. 
* _News_: El tamaño de los datos es aproximadamente de 1,010,242, proviene de un noticiero en línea estadounidense. El tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés. No se observan emoticones pero sí links y caracteres especiales, así como los signos dólar.
* _Twitter_: El tamaño de estos datos son de aproximadamente 2,360,148 palabras, proviene de la red social twitter, donde el contenido es especificamente estadounidense. Además, el tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés; se encuentran emoticones, tales como corazones y caras escritas mediante los signos otográficos, signos de interrogación y exclamación, hashtags # y menciones @.

## Limpieza y preprocesamiento
El objetivo de este apartado es limpiar la muestra de 5% de los 3 textos usados: blogs, news y twitter. Los paquetes que se usaron para este laboratorio fueron los siguientes:
```{r}
#install.packages("wordcloud")
#install.packages("slam") 
#install.packages("RColorBrewer")
#install.packages("devtools")
library("readr") #Lee los archivos 
library("tm")  #Contiene tranformaciones para el text mining
library("SnowballC") #Stopwords
library("wordcloud")
library("RColorBrewer") #para las nubes de palabras 
library(ggplot2) #para gráficos
library(dplyr) #manipualacion de texto
library(devtools) #Para instalar ngramas
#install_github("wrathematics/ngram")#descarga del paquete ngram
  library(ngram) #para ngramas
```

Ahora, se importan los datos, se leen y se crean samples aleatorias con 5% de las observaciones para cada texto a excepción de Twitter dado que es muy pesado y consume mucha memoria. Para twitter se crea un sample aleatorio con el 4%. Esto se hizo con el fin de reducir el tamaño de la muestra, ya que cada archivo de texto pesa demasiado y dificulta la hora de correr el programa. Cabe mencionar lo siguiente: 

* _texto[[1]]$content_: Hace referencia al archivo de Blogs. 
* _texto[[2]]$content_: Hace referencia al archivo de News. 
* _texto[[3]]$content_: Hace referencia al archivo de Twitter. 
```{r}
#-------------------------------------------------
# Directorios para de cada integrante del grupo

#C:/Users/smayr/Documents/Tercer año/Semestre 6/Data Science/Laboratorio 5
# /Users/quiebres/Desktop/Texts
#/Users/odalisrg/Downloads/Textos
#-------------------------------------------------

texto <- VCorpus(DirSource("/Users/quiebres/Desktop/Texts"), readerControl = list(language = "en"))

# Se deja fijo la muestra aleatoria
set.seed(3)
porciento <- 0.05
porcientoTwitter <- 0.03
texto[[1]]$content <- sample(texto[[1]]$content,length(texto[[1]]$content)*porciento)
texto[[2]]$content <- sample(texto[[2]]$content,length(texto[[2]]$content)*porciento)
texto[[3]]$content <- sample(texto[[3]]$content,length(texto[[3]]$content)*porcientoTwitter)

# Asignación de nuevas variables
blog <- Corpus(VectorSource(texto[[1]]$content))
news <- Corpus(VectorSource(texto[[2]]$content))
twitter <- Corpus(VectorSource(texto[[3]]$content))
```

Se realiza una vista previa para cada archivo, con el fin de saber la cantidad de palabras que hay en cada muestra y verificar que la extracción de datos aleatorios se haya realizado correctamente para las muestras del 5%. Se observa que para **blogs**, quedaron 44,964 palabras; para **news**, quedaron 50,521 palabras; para **twitter**, quedaron 118,007 palabras.
```{r}
#inspect(blog) 
#inspect(news)
#inspect(twitter)
```

Se cambian las letras a minúsculas, así se logra obtener un texto más homogéneo. 
```{r}
blog <- tm_map(blog, tolower)

news <- tm_map(news,tolower)

twitter <- tm_map(twitter, tolower)

```

Se eliminan los emoticones, las url, usuario de twitter y hashtags que aparecen en los textos. Se usó la función de gsub() para facilitar y ahorrar el tema de los espacios. A continuación se explican a detalle de la definición dentro de gsub():

* *[\^\x01-\x7F]*: En este caso, el ^ significa que el gsub() va a borrar todo excepto lo que sigue a continuación: \x01-\x7F, es decir, borrará todo excepto los caracteres del código ASCII. Basicamente esta función ayuda a eliminar emoticones.  
* *http[[:alnum:][:punct:]]*: Busca las palabras que empiecen con http y elimina los caracteres que siguen después de él, incluyendo signos de puntuación.
* *@[[:alnum:][:punct:]]$*: Busca las palabras que empiecen con una arroba (una mención, en el caso de twitter) y elimina los caracteres que siguen después de ello, incluyendo signos de puntuación. 
* *#[^[:space:]]*: Busca los hashtags (palabras que empiecen con el #) y las elimina. En este caso no es necesario colocar los signos de puntuación. 
```{r}
for (j in seq(blog)) {
  blog[[j]] <- gsub("[^\x01-\x7F]", "", blog[[j]])
  blog[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", blog[[j]])
  blog[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", blog[[j]])
  blog[[j]] <- gsub("#[^[:space:]]*", "", blog[[j]])
}


for (j in seq(news)) {
  news[[j]] <- gsub("[^\x01-\x7F]", "", news[[j]])
  news[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", news[[j]])
  news[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", news[[j]])
  news[[j]] <- gsub("#[^[:space:]]*", "", news[[j]])
}


for (j in seq(twitter)) {
  twitter[[j]] <- gsub("[^\x01-\x7F]", "", twitter[[j]])
  twitter[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", twitter[[j]])
  twitter[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", twitter[[j]])
  twitter[[j]] <- gsub("#[^[:space:]]*", "", twitter[[j]])
}
```

Se elminan todos los artículos y preposiciones con la función de *stopwords()*. Esta función permite eliminar aquellas palabras que están juntadas por apóstrofes en el idioma inglés. 
```{r}
blog <- tm_map(blog, removeWords, stopwords("english"))

news <- tm_map(news, removeWords, stopwords("english"))

twitter <- tm_map(twitter, removeWords, stopwords("english"))
```

Se decidió eliminar los signos de puntuación mediante la función **gsub** ya que la función removePuntuacion() del paquete tm, lo que hace es que no solo elimina los caracteres especiales, sino que también el espacio entre las palabras, siempre y cuando los signos de puntuación estén pegados a las palabras y no hayan espacios. Cabe mencionar que está función elimina también los caracteres especiales, tales como $ @ / # etc. 
```{r}
for (j in seq(blog)) {
    blog[[j]] <- gsub("[[:punct:]]"," ", blog[[j]])
}

for (j in seq(news)) {
    news[[j]] <- gsub("[[:punct:]]"," ", news[[j]])
}

for (j in seq()) {
    twitter[[j]] <- gsub("[[:punct:]]"," ", twitter[[j]])
}
```

También se decidió eliminar las conjugaciones de las palabras (pasado, futuro, gerundios, etc) y se dejó la palabra escrita en el presente. Se le aplica a todo el corpus de las muestras con la ayuda de la función tm_map().
```{r}
blog <- tm_map(blog, stemDocument)

news <- tm_map(news, stemDocument)

twitter <- tm_map(twitter, stemDocument)
```

Por último, se eliminan los números, ya que consideramos que interferirán en las predicciones; es de suma importancia lograr predecir las primeras tres palabras y consideramos que es más sencillo si eliminamos los números. 
```{r}
blog <- tm_map(blog, removeNumbers)

news <- tm_map(news, removeNumbers)

twitter <- tm_map(twitter, removeNumbers)
```
# Análisis Exploratorio
Obteniendo las palabras más frecuentes, se tomó la decisión de encontrar las que se encuentren por lo menos más de 1000 veces. Por otro lado, utilizamos la función removeSparseTerms() para deshacernos de las palabras más infrecuentes. Esto se hace con el motivo de poder disminuir el peso de mb para cada matriz
```{r}
#Se convierten en una document term matrix

dtmBlog <- DocumentTermMatrix(blog)

#Se convierten en un term document matrix
#blogM <-  TermDocumentMatrix(blog)

findFreqTerms(dtmBlog, lowfreq=1000)
```
Se observa que para el archivo de texto de Blog, las palabras más frecuentes que nos llamaron la atención fueron 'book', 'blog' dado que hacen referencia a que se pueden hacer reviews de libros o blogs, 'love' puede estar relacionado con análisis de emociones. 

```{r}
#Se convierten en una document term matrix

dtmTwitter <- DocumentTermMatrix(twitter)

#Se convierten en un term document matrix

#twitterM <- TermDocumentMatrix(twitter)


findFreqTerms(dtmTwitter, lowfreq=1000)
```
En este caso, para el archivo de texto de Twitter se tiene que las palabras más frecuentes que llaman la atención son: 'tweet' dado que inferimos que el usuario hace referencia a los tweets de alguien más. Por otro lado, encontramos palabras que se pueden asociar a emociones y sentimientos como: 'hope', 'miss', 'love', 'happiness'.

```{r}
#Se convierten en una document term matrix

dtmNews <- DocumentTermMatrix(news)

#Se convierten en un term document matrix

#newsM <- TermDocumentMatrix(news)


findFreqTerms(dtmNews, lowfreq=100)
```

Para el caso del documento de News se tiene que se tuvo que disminuir la frecuencia mínima a encontrar, en este caso se disminuyó a 100 palabras. Entre las que nos llamaron la atención fueron: 'office', 'public', 'money', 'company', 'million' ya que se pueden asociar a noticias sobre el ámbito laboral. 
```{r}

#Blog

wordcloud(blog, max.words=50, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Se observa que para el documento de Blog, las palabras más utilizadas son: 'one', 'like', 'time', 'will', 'day'. 


Cabe mencionar que en general, entre las palabras que se encuentran en todos los archivos y que más se repiten son: 'like', 'day', 'year' y 'said'. 

```{r}

#twitter

wordcloud(twitter, max.words=50, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Para el archivo de Twitter, se destaca nuevamente la palabra 'will' y 'come', pero también destaca 'thank', 'just', 'get' y 'love' ¿Se podría asociar estas palabras a sentimientos positivos?
Otras palabras que no destacan tanto, pero se encuentran en la nube de palabras son 'lol' y 'follow'.

Se prosigue a realizar los histogramas de frecuencia de palabras. Se convierte

```{r}

#News

wordcloud(news, max.words=80, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Para el archivo de News se tiene que la palabra que más se destaca es la de 'said', lo cual tiene sentido dado que en las noticias hacen referencia a entrevistas de personas involucradas en hechos. Otras de las palabras que se repiten, comparando con los archivos anteriores, son 'will' y 'one'. 

Se obtienen los histogramas de frecuencia mayores a 2000 de palabras por cada texto procesado
```{r}
# blog
freqBlog <- colSums(as.matrix(dtmBlog))

wfBlog <- data.frame(word=names(freqBlog), freq=freqBlog)
head(wfBlog)
```
```{r}
blogP <- ggplot(subset(wfBlog, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
blogP
```
```{r}
# twitter
freqTwitter <- colSums(as.matrix(dtmTwitter))

wfTwitter <- data.frame(word=names(freqTwitter), freq=freqTwitter)
head(wfTwitter)
```

```{r}
twitterP <- ggplot(subset(wfTwitter, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
twitterP
```
```{r}
# News
freqNews <- colSums(as.matrix(dtmNews))

wfNews <- data.frame(word=names(freqNews), freq=freqNews)
head(wfNews)
```
```{r}
newsP <- ggplot(subset(wfNews, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
newsP
```

# N - gramas

Se comienza creando un string para cada corpus, para luego poder ser aplicado el n-grama
```{r}
strBlogs <- concatenate(lapply(blog, "[", 1))

strTwitter <- concatenate(lapply(twitter, "[", 1))

strNews <- concatenate(lapply(news, "[", 1))

```
Se crean los bigramas para cada corpus. 

## Bigramas del archivo Blogs

```{r}

#Bigrama de Blogs

bgBlogs <- ngram(strBlogs, n=2)

get.phrasetable(bgBlogs)
```
Se encuentra que US es uno de los bigramas más utilizados, así como 'New York', también 'look like' lo cual nos lleva a pensar que probablemente se realizan muchas comparaciones en este archivo.


## Bigramas del archivo Twitter
```{r}

#Bigrama de Twitter

bgTwitter <- ngram(strTwitter, n=2)

get.phrasetable(bgTwitter)
```
Era de esperarse que 'rt' fuera uno de los bigramas más utilizados, nuevamente se encuentra la frase 'look like' como una de las más frecuentes.


## Bigramas del archivo News
```{r}

#Bigrama de News

bgNews <- ngram(strNews, n=2)

get.phrasetable(bgNews)
```
En este caso, 'Us' vuelve a ser de las más frecuentes, otros bigramas que nos llaman la atención son 'St. Louis', 'New York' y 'New Jersey' ¿podría ser que las noticias más frecuentes estén relacionadas con suscesos en esas 3 ciudades?


A continuación se encontrarán los trigramas


## Trigramas del archivo Blogs
```{r}

#Trigrama de Blogs

tgBlogs <- ngram(strBlogs, n=3)

get.phrasetable(tgBlogs)
```
Algo que cabe destacar en estos trigramas es que 'world war ii' es de los más frecuentes así como ' nuclear power plant', ¿Acaso en los blogs, en su mayoría, se habla de la Segunda Guerra Mundial?


## Trigramas del archivo Twitter
```{r}

#Trigrama de Twitter

tgTwitter <- ngram(strTwitter, n=3)

get.phrasetable(tgTwitter)
```




## Trigramas del archivo News
```{r}

#Trigrama de News

tgNews <- ngram(strNews, n=3)

get.phrasetable(tgNews)
```
Entre los más destacados son el de 'president Barack Obama' porque nos lleva a pensar que posiblemente las noticias sean correspondientes a los años en los que Barack Obama fue presidente de Estados Unidos, asimismo ' U S attorney' ya que se podrían relacionar las noticias con asuntos legales. 





