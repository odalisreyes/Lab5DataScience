---
title: "Laboratorio 5"
output:
  html_notebook: default
  pdf_document: default
---

## Descripción de los datos
* _Blogs_: El tamaño de estos datos son de 899,288 palabras, proviene de un blog estadounidense. Además, el tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés; se encuentran emoticones, tales como corazones y caras escritas mediante los signos ortográficos. Aparecen muchas veces los signos de exclamación y se observa una escritura de narración para un público abierto. 
* _News_: El tamaño de los datos es aproximadamente de 1,010,242, proviene de un noticiero en línea estadounidense. El tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés. No se observan emoticones pero sí links y caracteres especiales, así como los signos dólar.
* _Twitter_: El tamaño de estos datos son de aproximadamente 2,360,148 palabras, proviene de la red social twitter, donde el contenido es especificamente estadounidense. Además, el tipo de corpus es monolingüe, ya que el texto se encuentra escrito solamente en inglés; se encuentran emoticones, tales como corazones y caras escritas mediante los signos otográficos, signos de interrogación y exclamación, hashtags # y menciones @.

## Limpieza y preprocesamiento
El objetivo de este apartado es limpiar la muestra de 5% de los 3 textos usados: blogs, news y twitter. Los paquetes que se usaron para este laboratorio fueron los siguientes:
```{r}
#install.packages("wordcloud")
#install.packages("slam") 
#install.packages("RColorBrewer")
#install.packages("devtools")
library("readr") #Lee los archivos 
library("tm")  #Contiene tranformaciones para el text mining
library("SnowballC") #Stopwords
library("wordcloud")
library("RColorBrewer") #para las nubes de palabras 
library(ggplot2) #para gráficos
library(dplyr) #manipualacion de texto
library(devtools) #Para instalar ngramas
#install_github("wrathematics/ngram")#descarga del paquete ngram
library(ngram) #para ngramas
library(data.table)
library(quanteda)
```

Ahora, se importan los datos, se leen y se crean samples aleatorias con 5% de las observaciones para cada texto a excepción de Twitter dado que es muy pesado y consume mucha memoria. Para twitter se crea un sample aleatorio con el 4%. Esto se hizo con el fin de reducir el tamaño de la muestra, ya que cada archivo de texto pesa demasiado y dificulta la hora de correr el programa. Cabe mencionar lo siguiente: 

* _texto[[1]]$content_: Hace referencia al archivo de Blogs. 
* _texto[[2]]$content_: Hace referencia al archivo de News. 
* _texto[[3]]$content_: Hace referencia al archivo de Twitter. 
```{r}
#-------------------------------------------------
# Directorios para de cada integrante del grupo

#C:/Users/smayr/Documents/Tercer año/Semestre 6/Data Science/Laboratorio 5
# /Users/quiebres/Desktop/Texts
#/Users/odalisrg/Downloads/Textos
#-------------------------------------------------

texto <- VCorpus(DirSource("/Users/odalisrg/Downloads/Textos"), readerControl = list(language = "en"))

# Se deja fijo la muestra aleatoria
set.seed(3)
porciento <- 0.05
porcientoTwitter <- 0.03
texto[[1]]$content <- sample(texto[[1]]$content,length(texto[[1]]$content)*porciento)
texto[[2]]$content <- sample(texto[[2]]$content,length(texto[[2]]$content)*porciento)
texto[[3]]$content <- sample(texto[[3]]$content,length(texto[[3]]$content)*porcientoTwitter)

# Asignación de nuevas variables
blog <- Corpus(VectorSource(texto[[1]]$content))
news <- Corpus(VectorSource(texto[[2]]$content))
twitter <- Corpus(VectorSource(texto[[3]]$content))
```

Se realiza una vista previa para cada archivo, con el fin de saber la cantidad de palabras que hay en cada muestra y verificar que la extracción de datos aleatorios se haya realizado correctamente para las muestras del 5%. Se observa que para **blogs**, quedaron 44,964 palabras; para **news**, quedaron 50,521 palabras; para **twitter**, quedaron 118,007 palabras.
```{r}
#inspect(blog) 
#inspect(news)
#inspect(twitter)
```

### Letras mayúsculas a minúsculas
Se cambian las letras a minúsculas, así se logra obtener un texto más homogéneo. 
```{r}
blog <- tm_map(blog, tolower)

news <- tm_map(news,tolower)

twitter <- tm_map(twitter, tolower)

```

### Eliminación de caracteres especiales, emoticones y url
Se eliminan los emoticones, las url, usuario de twitter y hashtags que aparecen en los textos. Se usó la función de gsub() para facilitar y ahorrar el tema de los espacios. A continuación se explican a detalle de la definición dentro de gsub():

* *[\^\x01-\x7F]*: En este caso, el ^ significa que el gsub() va a borrar todo excepto lo que sigue a continuación: \x01-\x7F, es decir, borrará todo excepto los caracteres del código ASCII. Basicamente esta función ayuda a eliminar emoticones.  
* *http[[:alnum:][:punct:]]*: Busca las palabras que empiecen con http y elimina los caracteres que siguen después de él, incluyendo signos de puntuación.
* *@[[:alnum:][:punct:]]$*: Busca las palabras que empiecen con una arroba (una mención, en el caso de twitter) y elimina los caracteres que siguen después de ello, incluyendo signos de puntuación. 
* *#[^[:space:]]*: Busca los hashtags (palabras que empiecen con el #) y las elimina. En este caso no es necesario colocar los signos de puntuación. 
```{r}
for (j in seq(blog)) {
  blog[[j]] <- gsub("[^\x01-\x7F]", "", blog[[j]])
  blog[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", blog[[j]])
  blog[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", blog[[j]])
  blog[[j]] <- gsub("#[^[:space:]]*", "", blog[[j]])
}


for (j in seq(news)) {
  news[[j]] <- gsub("[^\x01-\x7F]", "", news[[j]])
  news[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", news[[j]])
  news[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", news[[j]])
  news[[j]] <- gsub("#[^[:space:]]*", "", news[[j]])
}


for (j in seq(twitter)) {
  twitter[[j]] <- gsub("[^\x01-\x7F]", "", twitter[[j]])
  twitter[[j]] <- gsub("http[[:alnum:][:punct:]]*", "", twitter[[j]])
  twitter[[j]] <- gsub("@[[:alnum:][:punct:]]*", "", twitter[[j]])
  twitter[[j]] <- gsub("#[^[:space:]]*", "", twitter[[j]])
}
```

### Eliminación de stopwords
Se elminan todos los artículos y preposiciones con la función de *stopwords()*. Esta función permite eliminar aquellas palabras que están juntadas por apóstrofes en el idioma inglés. 
```{r}
blog <- tm_map(blog, removeWords, stopwords("english"))

news <- tm_map(news, removeWords, stopwords("english"))

twitter <- tm_map(twitter, removeWords, stopwords("english"))
```

### Eliminación de signos de puntuación
Se decidió eliminar los signos de puntuación mediante la función **gsub** ya que la función removePuntuacion() del paquete tm, lo que hace es que no solo elimina los caracteres especiales, sino que también el espacio entre las palabras, siempre y cuando los signos de puntuación estén pegados a las palabras y no hayan espacios. Cabe mencionar que está función elimina también los caracteres especiales, tales como $ @ / # etc. 
```{r}
#for (j in seq(blog)) {
#    blog[[j]] <- gsub("[[:punct:]]"," ", blog[[j]])
#}

#for (j in seq(news)) {
#    news[[j]] <- gsub("[[:punct:]]"," ", news[[j]])
#}

#for (j in seq()) {
#    twitter[[j]] <- gsub("[[:punct:]]"," ", twitter[[j]])
#}

blog <- tm_map(blog, removePunctuation)

news <- tm_map(news, removePunctuation)

twitter <- tm_map(twitter, removePunctuation)

```

### Eliminación de conjugaciones de palabras
También se decidió eliminar las conjugaciones de las palabras (pasado, futuro, gerundios, etc) y se dejó la palabra escrita en el presente. Se le aplica a todo el corpus de las muestras con la ayuda de la función tm_map().
```{r}
#blog <- tm_map(blog, stemDocument)

#news <- tm_map(news, stemDocument)

#twitter <- tm_map(twitter, stemDocument)
```
Cuando realizamos este procedimiento, nos dimos cuenta que al momento de sacar los n-gramas, las palabras perdían un poco de léxico y se creaban faltas ortográficas (e.g., la palabra _happy_ se convertía en _happi_). Por eso mismo, decidimos no aplicar esta transformación. 

### Eliminación de números
Por último, se eliminan los números, ya que consideramos que interferirán en las predicciones; es de suma importancia lograr predecir las primeras tres palabras y consideramos que es más sencillo si eliminamos los números. 
```{r}
blog <- tm_map(blog, removeNumbers)

news <- tm_map(news, removeNumbers)

twitter <- tm_map(twitter, removeNumbers)
```
## Análisis Exploratorio
Obteniendo las palabras más frecuentes, se tomó la decisión de encontrar las que se encuentren por lo menos más de 1000 veces. Por otro lado, utilizamos la función removeSparseTerms() para deshacernos de las palabras más infrecuentes. Esto se hace con el motivo de poder disminuir el peso de MB para cada matriz.
```{r}
#Se convierten en una document term matrix

dtmBlog <- DocumentTermMatrix(blog)

#Se convierten en un term document matrix
#blogM <-  TermDocumentMatrix(blog)

findFreqTerms(dtmBlog, lowfreq=1000)
```
Se observa que para el archivo de texto de Blog, las palabras más frecuentes que nos llamaron la atención fueron 'book', 'blog' dado que hacen referencia a que se pueden hacer reviews de libros o blogs, 'love' puede estar relacionado con análisis de emociones. 

```{r}
#Se convierten en una document term matrix

dtmTwitter <- DocumentTermMatrix(twitter)

#Se convierten en un term document matrix

#twitterM <- TermDocumentMatrix(twitter)


findFreqTerms(dtmTwitter, lowfreq=1000)
```
En este caso, para el archivo de texto de Twitter se tiene que las palabras más frecuentes que llaman la atención son: 'tweet' dado que inferimos que el usuario hace referencia a los tweets de alguien más. Por otro lado, encontramos palabras que se pueden asociar a emociones y sentimientos como: 'hope', 'miss', 'love', 'happiness'.

```{r}
#Se convierten en una document term matrix

dtmNews <- DocumentTermMatrix(news)

#Se convierten en un term document matrix

#newsM <- TermDocumentMatrix(news)


findFreqTerms(dtmNews, lowfreq=100)
```
Para el caso del documento de News se tiene que se tuvo que disminuir la frecuencia mínima a encontrar, en este caso se disminuyó a 100 palabras. Entre las que nos llamaron la atención fueron: 'office', 'public', 'money', 'company', 'million' ya que se pueden asociar a noticias sobre el ámbito laboral. 

### Nube de palabras
#### Archivo Blogs
```{r}

#Blog

wordcloud(blog, max.words=50, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Se observa que para el documento de Blog, las palabras más utilizadas son: 'one', 'like', 'time', 'will', 'day'. 
Cabe mencionar que en general, entre las palabras que se encuentran en todos los archivos y que más se repiten son: 'like', 'day', 'year' y 'said'. 

#### Archivo Twitter
```{r}

#twitter

wordcloud(twitter, max.words=50, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Para el archivo de Twitter, se destaca nuevamente la palabra 'will' y 'come', pero también destaca 'thank', 'just', 'get' y 'love' ¿Se podría asociar estas palabras a sentimientos positivos?
Otras palabras que no destacan tanto, pero se encuentran en la nube de palabras son 'lol' y 'follow'.

#### Archivo News
```{r}

#News

wordcloud(news, max.words=80, random.order= FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
Para el archivo de News se tiene que la palabra que más se destaca es la de 'said', lo cual tiene sentido dado que en las noticias hacen referencia a entrevistas de personas involucradas en hechos. Otras de las palabras que se repiten, comparando con los archivos anteriores, son 'will' y 'one'. 

### Histogramas
Se prosigue a realizar los histogramas de frecuencia de palabras. Se obtienen los histogramas de frecuencia mayores a 2000 de palabras por cada texto procesado. 
#### Archivo blogs
```{r}
# blog
freqBlog <- colSums(as.matrix(dtmBlog))

wfBlog <- data.frame(word=names(freqBlog), freq=freqBlog)
head(wfBlog)
```
```{r}
blogP <- ggplot(subset(wfBlog, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
blogP
```

#### Archivo Twitter
```{r}
# twitter
freqTwitter <- colSums(as.matrix(dtmTwitter))

wfTwitter <- data.frame(word=names(freqTwitter), freq=freqTwitter)
head(wfTwitter)
```

```{r}
twitterP <- ggplot(subset(wfTwitter, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
twitterP
```

#### Archivo News
```{r}
# News
freqNews <- colSums(as.matrix(dtmNews))

wfNews <- data.frame(word=names(freqNews), freq=freqNews)
head(wfNews)
```
```{r}
newsP <- ggplot(subset(wfNews, freq>2000), aes(x = reorder(word, -freq), y = freq)) +
          geom_bar(stat = "identity") + 
          theme(axis.text.x=element_text(angle=45, hjust=1))
newsP
```

## N-gramas
Se comienza creando un string para cada corpus, para luego poder ser aplicado el n-grama
```{r}
strBlogs <- concatenate(lapply(blog, "[", 1))

strTwitter <- concatenate(lapply(twitter, "[", 1))

strNews <- concatenate(lapply(news, "[", 1))

```
Se crean los bigramas para cada corpus. 

### Bigramas del archivo Blogs

```{r}

#Bigrama de Blogs

bgBlogs <- ngram(strBlogs, n=2)

get.phrasetable(bgBlogs)
```
Se encuentra que US es uno de los bigramas más utilizados, así como 'New York', también 'look like' lo cual nos lleva a pensar que probablemente se realizan muchas comparaciones en este archivo.


### Bigramas del archivo Twitter
```{r}

#Bigrama de Twitter

bgTwitter <- ngram(strTwitter, n=2)

get.phrasetable(bgTwitter)
```
Era de esperarse que 'rt' fuera uno de los bigramas más utilizados, nuevamente se encuentra la frase 'look like' como una de las más frecuentes.


### Bigramas del archivo News
```{r}

#Bigrama de News

bgNews <- ngram(strNews, n=2)

get.phrasetable(bgNews)
```
En este caso, 'Us' vuelve a ser de las más frecuentes, otros bigramas que nos llaman la atención son 'St. Louis', 'New York' y 'New Jersey' ¿podría ser que las noticias más frecuentes estén relacionadas con suscesos en esas 3 ciudades?


A continuación se encontrarán los trigramas


### Trigramas del archivo Blogs
```{r}

#Trigrama de Blogs

tgBlogs <- ngram(strBlogs, n=3)

get.phrasetable(tgBlogs)
```
Algo que cabe destacar en estos trigramas es que 'world war ii' es de los más frecuentes así como ' nuclear power plant', ¿Acaso en los blogs, en su mayoría, se habla de la Segunda Guerra Mundial?


### Trigramas del archivo Twitter
```{r}

#Trigrama de Twitter

tgTwitter <- ngram(strTwitter, n=3)

get.phrasetable(tgTwitter)
```




### Trigramas del archivo News
```{r}

#Trigrama de News

tgNews <- ngram(strNews, n=3)

get.phrasetable(tgNews)
```
Entre los más destacados son el de 'president Barack Obama' porque nos lleva a pensar que posiblemente las noticias sean correspondientes a los años en los que Barack Obama fue presidente de Estados Unidos, asimismo ' U S attorney' ya que se podrían relacionar las noticias con asuntos legales. 

## Probabilidades de palabras y Kneser-Kney Smoothing
Para realizar el algoritmo de Kneser-Kney Smoothing, proseguimos a unir los 3 datasets limpios a uno solo. Luego, lo convertimos a un corpus. Esto se hizo con el fin que al momento de estar aplicando el algoritmo de predicción con los ngramas, se tuviera una mayor base de datos de donde sacar información para las predicciones. 
```{r}
# Conversión a un solo data set
full.text <- c(blog, news, twitter)

# Convirtiendo a un corpus
full.data <- Corpus(VectorSource(full.text))

# Se convierte a token
data.token <- tokens(
    x = tolower(full.data),
    remove_punct = FALSE,
    remove_twitter = FALSE,
    remove_numbers = FALSE,
    remove_hyphens = FALSE,
    remove_symbols = FALSE,
    remove_url = FALSE
)


# Unigrama, bigrama y trigrama del texto completo
bigram <- tokens_ngrams(data.token, n = 2)
trigram <- tokens_ngrams(data.token, n = 3)

# Se crea una DFM
uni <- dfm_trim(dfm(data.token), 3)
bi <- dfm_trim(dfm(bigram), 3)
tri <- dfm_trim(dfm(trigram), 3)

# Se crean vectores con el contador de palabras
sumsU <- colSums(uni)
sumsB <- colSums(bi)
sumsT <- colSums(tri)

# Se crean tablas con columnas de cada palabra del uni/bi/tri-grama
unigram.pal <- data.table(word_1 = names(sumsU), count = sumsU)

bigram.pal <- data.table(
        word_1 = sapply(strsplit(names(sumsB), "_", fixed = TRUE), '[[', 1),
        word_2 = sapply(strsplit(names(sumsB), "_", fixed = TRUE), '[[', 2),
        count = sumsB)

trigram.pal <- data.table(
        word_1 = sapply(strsplit(names(sumsT), "_", fixed = TRUE), '[[', 1),
        word_2 = sapply(strsplit(names(sumsT), "_", fixed = TRUE), '[[', 2),
        word_3 = sapply(strsplit(names(sumsT), "_", fixed = TRUE), '[[', 3),
        count = sumsT)

# Se colocan indices a los ngramas
setkey(unigram.pal, word_1)
setkey(bigram.pal, word_1, word_2)
setkey(trigram.pal, word_1, word_2, word_3)

```

### Probabilidad de los uni usando KNP
```{r}
# Finding the most frequently used 50 unigrmas
unigram.pal <- unigram.pal[order(-Prob)][1:50]
```

### Probabilidad de los bigramas usando KNP
Con este algoritmo, proseguimos a calcular la probabilidad de las palabras de cada ngrama realizado. 
```{r}
valordes <- 0.75

######## Finding Bi-Gram Probability #################

# Finding number of bi-gram words
numOfBiGrams <- nrow(bigram.pal[by = .(word_1, word_2)])

# Dividing number of times word 2 occurs as second part of bigram, by total number of bigrams.  
# ( Finding probability for a word given the number of times it was second word of a bigram)
ckn <- bigram.pal[, .(Prob = ((.N) / numOfBiGrams)), by = word_2]
setkey(ckn, word_2)

# Assigning the probabilities as second word of bigram, to unigrams
unigram.pal[, Prob := ckn[word_1, Prob]]
unigram.pal <- unigram.pal[!is.na(unigram.pal$Prob)]

# Finding number of times word 1 occurred as word 1 of bi-grams
n1wi <- bigram.pal[, .(N = .N), by = word_1]
setkey(n1wi, word_1)

# Assigning total times word 1 occured to bigram cn1
bigram.pal[, Cn1 := unigram.pal[word_1, count]]

# Kneser Kney Algorithm
bigram.pal[, Prob := ((count - valordes) / Cn1 + valordes / Cn1 * n1wi[word_1, N] * unigram.pal[word_2, Prob])]

######## End of Finding Bi-Gram Probability #################
```

### Probabilidad de los trigramas usando KNP
```{r}
######## Finding Tri-Gram Probability #################

# Finding count of word1-word2 combination in bigram 
trigram.pal[, Cn2 := bigram.pal[.(word_1, word_2), count]]

# Finding count of word1-word2 combination in trigram
n1w12 <- trigram.pal[, .N, by = .(word_1, word_2)]
setkey(n1w12, word_1, word_2)

# Kneser Kney Algorithm
trigram.pal[, Prob := (count - valordes) / Cn2 + valordes / Cn2 * n1w12[.(word_1, word_2), N] *
              bigram.pal[.(word_1, word_2), Prob]]

######## End of Finding Tri-Gram Probability #################
```


## Algoritmo de predicción 
```{r}
# function to return highly probable previous word given a word
biWords <- function(w1, n = 5) {
    pwords <- bigram.pal[w1][order(-Prob)]
    if (any(is.na(pwords)))
        return(uniWords(n))
    if (nrow(pwords) > n)
        return(pwords[1:n, word_2])
    count <- nrow(pwords)
    unWords <- uniWords(n)[1:(n - count)]
    return(c(pwords[, word_2], unWords))
}


# function to return random words from unigrams
uniWords <- function(n = 5) {  
    return(sample(unigram.pal[, word_1], size = n))
}
```



```{r}
# The prediction app
getWords <- function(str){
    require(quanteda)
    tokens <- tokens(x = char_tolower(str))
    tokens <- char_wordstem(rev(rev(tokens[[1]])[1:2]), language = "english")
    
    words <- triWords(tokens[1], tokens[2], 5)
    chain_1 <- paste(tokens[1], tokens[2], words[1], sep = " ")

    print(words[2])
}
```
